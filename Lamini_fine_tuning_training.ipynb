{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zakcroft/fine-tuning-notebooks/blob/main/Lamini_fine_tuning_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmjAlA5T_XMO"
      },
      "outputs": [],
      "source": [
        "!pip install datasets config transformers[torch] lamini evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOnf158m-DRu"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/deeplearning_fine_tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndBvkLEWzR1V"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "import config\n",
        "import os\n",
        "import yaml\n",
        "import logging\n",
        "import time\n",
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# from utilities import *\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NchNUm5b9QDV"
      },
      "outputs": [],
      "source": [
        "!ls\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc5ylTni4jcN"
      },
      "outputs": [],
      "source": [
        "device_count = torch.cuda.device_count()\n",
        "logger.debug(\"Checking device\")\n",
        "if device_count > 0:\n",
        "    print(\"Select GPU device\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Select CPU device\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHoRqexlCrSs"
      },
      "outputs": [],
      "source": [
        "model_name = \"EleutherAI/pythia-70m\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "base_model.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXLvx33v3seD"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"lamini/lamini_docs\"\n",
        "\n",
        "dataset = datasets.load_dataset(dataset_path)\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "print(train_dataset[0]['question'])\n",
        "print(train_dataset[0]['answer'])\n",
        "\n",
        "print(test_dataset[0]['question'])\n",
        "print(test_dataset[0]['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpJakYlADaSQ"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"question\"], padding=True, truncation=True,  return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "small_train_dataset = dataset[\"train\"].select(range(1))\n",
        "small_test_dataset = dataset[\"test\"].select(range(1))\n",
        "\n",
        "# print(train_dataset)\n",
        "print(small_train_dataset[0])\n",
        "print(small_test_dataset[0])\n",
        "\n",
        "# encode\n",
        "encoding_dataset = small_test_dataset.map(tokenize_function, batched=True)\n",
        "input_ids=torch.tensor(encoding_dataset['input_ids']).to(device)\n",
        "attention_mask = torch.tensor(encoding_dataset['attention_mask']).to(device)\n",
        "\n",
        "# ask\n",
        "base_model_generated_tokens_with_prompt = base_model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=512\n",
        "  )\n",
        "\n",
        "# decode\n",
        "generated_text_with_prompt = tokenizer.batch_decode(base_model_generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "print(generated_text_with_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lryF_OreCxeB"
      },
      "outputs": [],
      "source": [
        "# print(\"Question input:\", small_train_dataset[0]['question'])\n",
        "# print(\"Correct answer from Lamini docs:\", small_train_dataset[0]['answer'])\n",
        "\n",
        "# # Strip the prompt\n",
        "base_model_generated_answer = generated_text_with_prompt[0][len(small_test_dataset[0]['question']):].replace('.', '.\\n')\n",
        "\n",
        "print('Models answer:', base_model_generated_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixQNiai9TuGH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Now train\n",
        "\n",
        "max_steps = -1\n",
        "epochs=2\n",
        "batch_size=1\n",
        "\n",
        "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
        "output_dir = trained_model_name\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=epochs,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=max_steps,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=batch_size,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=120, # Number of update steps between two evaluations\n",
        "  save_steps=120, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  evaluation_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")\n",
        "\n",
        "model_flops = (\n",
        "  base_model.floating_point_ops(\n",
        "    {\n",
        "       \"input_ids\": torch.zeros(\n",
        "           (1, 2048)\n",
        "      )\n",
        "    }\n",
        "  )\n",
        "  * training_args.gradient_accumulation_steps\n",
        ")\n",
        "\n",
        "# print(base_model)\n",
        "# print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
        "# print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mDxK-b5Nj6F"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv870GZTLBkJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # Assumes `tokenizer` is previously defined\n",
        "\n",
        "\n",
        "class FilteredDataset(Dataset):\n",
        "    def __init__(self, original_dataset):\n",
        "        self.filtered_data = [item for item in original_dataset if len(item['input_ids']) > 0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.filtered_data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_data)\n",
        "small_train_dataset = dataset[\"train\"].select(range(230))\n",
        "small_test_dataset = dataset[\"train\"].select(range(230))\n",
        "filtered_train_dataset = FilteredDataset(small_train_dataset)\n",
        "filtered_test_dataset = FilteredDataset(small_test_dataset)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=filtered_train_dataset,\n",
        "    eval_dataset=filtered_test_dataset,\n",
        "    data_collator=data_collator,  # Add this line\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yPO2hy1Qugl"
      },
      "outputs": [],
      "source": [
        "training_output = trainer.train()\n",
        "\n",
        "print(training_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLZZY8MpVBId"
      },
      "outputs": [],
      "source": [
        "save_dir = f'{output_dir}/final'\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)\n",
        "\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
        "\n",
        "finetuned_model_output = finetuned_model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNRvbsm9VorY"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(10))\n",
        "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(10))\n",
        "\n",
        "# encode\n",
        "encoding_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
        "input_ids=torch.tensor(encoding_dataset['input_ids']).to(device)\n",
        "attention_mask = torch.tensor(encoding_dataset['attention_mask']).to(device)\n",
        "\n",
        "# ask\n",
        "finetuned_generated_tokens_with_prompt = finetuned_model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=512\n",
        "  )\n",
        "\n",
        "print(finetuned_generated_tokens_with_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decode\n",
        "finetuned_generated_text_with_prompt = tokenizer.batch_decode(finetuned_generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "print(\"Question input:\", small_train_dataset[0]['question'])\n",
        "print(\"Correct answer from Lamini docs:\", small_train_dataset[0]['answer'])\n",
        "print(finetuned_generated_text_with_prompt)\n",
        "# # Strip the prompt\n",
        "finetuned_generated_answer = finetuned_generated_text_with_prompt[0][len(small_train_dataset[0]['question']):].replace('?', '?\\n')\n",
        "\n",
        "print('Models answer:', finetuned_generated_answer)"
      ],
      "metadata": {
        "id": "fbdaBqBuqfAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VsPz1-qDVjfx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcx6zNe0VpAe"
      },
      "outputs": [],
      "source": [
        "# Strip the prompt\n",
        "finetuned_generated_text_answer = finetuned_generated_text_with_prompt[0][len(small_train_dataset[0]['question']):]\n",
        "\n",
        "base_model_modified_text = base_model_generated_answer.replace(\"?\", \"?\\n\")\n",
        "finetuned_modified_text = finetuned_generated_text_answer.replace(\"?\", \"?\\n\")\n",
        "\n",
        "print(base_model_modified_text)\n",
        "print('===============')\n",
        "print(finetuned_modified_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FM3XHTSLIGM"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1yy5cgFoQuagWrxPytG1bFjue9_-93wlb",
      "authorship_tag": "ABX9TyPHO/C0gILv0WO3vKLj9UMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}